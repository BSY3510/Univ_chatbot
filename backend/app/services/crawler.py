import json
import os
import time
from sqlalchemy.orm import Session

# DB ê´€ë ¨ ëª¨ë“ˆ
from app.db.session import SessionLocal, engine
from app.db.models import Post
from app.db import models

# ë¦¬íŒ©í† ë§ëœ ëª¨ë“ˆ
from app.services.crawler_utils import make_request, get_full_url, build_next_page_url
from app.services.crawler_parser import ContentParser

# DB í…Œì´ë¸” ìƒì„±
# models.Base.metadata.create_all(bind=engine)

# ì„¤ì • íŒŒì¼ ê²½ë¡œ
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
SITES_JSON_PATH = os.path.join(BASE_DIR, 'app', 'core', 'sites.json')

def load_sites_config() -> list:
    """
    app/core/sites.json íŒŒì¼ì—ì„œ í¬ë¡¤ë§í•  ì‚¬ì´íŠ¸ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    """
    try:
        with open(SITES_JSON_PATH, 'r', encoding='utf-8') as f:
            sites = json.load(f)
        return sites
    except Exception as e:
        print(f"ì„¤ì • íŒŒì¼({SITES_JSON_PATH}) ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

def save_post_to_db(db: Session, post_data: dict):
    """
    í¬ë¡¤ë§í•œ ê²Œì‹œë¬¼ ë”•ì…”ë„ˆë¦¬ë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤. (ì¤‘ë³µ ê²€ì‚¬ í¬í•¨)
    """
    try:
        # [ë³€ê²½ì ] ì¤‘ë³µ ê²€ì‚¬ë¥¼ ì—¬ê¸°ì„œ í•œ ë²ˆ ë” ìˆ˜í–‰ (ì•ˆì •ì„±ì„ ìœ„í•´)
        existing_post = db.query(Post.id).filter(
            Post.university == post_data['university'],
            Post.department == post_data['department'],
            Post.article_id == post_data['article_id']
        ).first()
        
        if existing_post:
            return

        new_post = Post(
            title=post_data['title'],
            content=post_data['content'],
            author=post_data['author'],
            created_at=post_data['date'],
            url=post_data['url'],
            article_id=post_data['article_id'],
            university=post_data['university'],
            department=post_data['department'],
            category=post_data['category']
        )
        
        db.add(new_post)
        db.commit()
        print(f"  -> ìƒˆ ê²Œì‹œë¬¼ ì €ì¥: {new_post.title}")

    except Exception as e:
        db.rollback()
        print(f"  -> DB ì €ì¥ ì˜¤ë¥˜: {e} (ê²Œì‹œë¬¼: {post_data.get('title')})")

def scrape_site(db: Session, site_config: dict):
    """
    (main_crawler.pyì˜ process_site ë¡œì§ì„ DB ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì •)
    """
    print(f"\nğŸ” [{site_config['name']}] í¬ë¡¤ë§ ì‹œì‘...")
    
    current_url = get_full_url(site_config['base_url'], site_config['start_url'])
    selectors = site_config['selectors']
    page_count = 1
    
    while current_url:
        print(f"  ... {page_count}í˜ì´ì§€ ì ‘ê·¼ ì¤‘: {current_url}")
        
        list_html = make_request(current_url) 
        if not list_html:
            print("  -> ëª©ë¡ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨. ì´ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break
            
        links = ContentParser.extract_links(list_html, selectors)
        if not links:
            print(f"  -> ì´ í˜ì´ì§€ì—ì„œ ê²Œì‹œë¬¼ ë§í¬({selectors.get('board_path')})ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
            
        new_post_found_on_this_page = False
        
        for relative_path, article_id in links:
            
            existing_post = db.query(Post.id).filter(
                Post.university == site_config['university'],
                Post.department == site_config['department'],
                Post.article_id == article_id
            ).first()
            
            if existing_post:
                continue 
            
            new_post_found_on_this_page = True

            detail_url = get_full_url(current_url, relative_path)
            
            detail_html = make_request(detail_url)
            if not detail_html:
                print(f"  -> ìƒì„¸ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {detail_url}")
                continue
                
            post_details = ContentParser.extract_details(detail_html, selectors)
            if not post_details:
                print(f"  -> ìƒì„¸ ì •ë³´ íŒŒì‹± ì‹¤íŒ¨: {detail_url}")
                continue
                
            post_data = {
                **post_details,
                "url": detail_url,
                "article_id": article_id,
                "university": site_config['university'],
                "department": site_config['department'],
                "category": site_config['category']
            }
            
            save_post_to_db(db, post_data)
            time.sleep(0.5) 
            
        if not new_post_found_on_this_page:
            print("  -> ì´ í˜ì´ì§€ì—ì„œ ìƒˆë¡œìš´ ê²Œì‹œë¬¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
            
        current_url = build_next_page_url(current_url, site_config)
        page_count += 1
        time.sleep(1)

def run_crawler():
    """
    ì „ì²´ í¬ë¡¤ë§ ì‘ì—…ì„ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    """
    sites = load_sites_config()
    if not sites:
        return

    db = SessionLocal()
    try:
        for site_config in sites:
            scrape_site(db, site_config)
    finally:
        db.close()
        print("\nDB ì„¸ì…˜ì„ ë‹«ì•˜ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ.")

if __name__ == "__main__":
    run_crawler()